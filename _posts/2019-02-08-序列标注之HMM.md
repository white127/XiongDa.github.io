---
layout: post
title: "序列标志之HMM（隐马尔科夫模型）"
subtitle: "Sequence Labeling - HMM"
author: "J."
header-img: "img/post-bg-infinity.jpg"
header-mask: 0.3
mathjax: true
tags:
  - 知乎
  - Machine Learning
  - 机器学习理论
  - 序列标注
  - HMM
  - 隐马尔科夫模型
---

# 前言

序列标注（Sequence Labeling）是机器学习的一大分支，特别在NLP领域用途更广，因为人类语言是由一系列的顺序符号组成，本身就是一个文字序列。序列标注可以应用
在分词、词性标志、实体识别等领域。

序列标注有多种算法，常见的HMM（隐马尔科夫）、CRF（条件随机场）。这两个模型（包括MEMM）有非常多的相似之处和细微的差别，网上的大量文章对此的描述多是通过
大量的文字和一些简单的公式或图片来说明，细节的部分表述不够清楚。要精确的理解他们的差异，需要从算法的数学理论出发。

下面详细介绍HMM模型。

# 符号定义

为了便于理解，下面将结合一个具体的序列标注应用场景--词性标注，来进行说明。
1. $X$：变量，表示一个句子（一个字序列）
2. $X^n$：表示集合中的第n个句子，由K个字组成，$X_1^n,X_2^n,...,X_k^n$
3. $X_k^n$：表示第n个句子的第k个字，字构成句子
4. $Y$：变量，表示一个词性序列
5. $Y^n$：表示集合中的第n个句子对应的词性序列，由K个词性组成，$Y_1^n,Y_2^n,...,Y_k^n$
6. $Y_k^n$：表示第n个句子的第k个字对应的词性，所有词性构成一个词性序列
6. $D$：表示整个数据集，包括所有句子和词性序列，${\{X^n,Y^n\}}_{n=1}^N$

# 建模
对序列标注（sequence labeling）模型进行讲解，通常会使用隐藏序列和观察序列，理解起来有点抽象，为了更好的结合实际案例来理解，本文采用词性标注的应用案例。那么，对序列标注模型的基本建模思路就是：  
定义一个次序列（也就是一个句子）$X=X_1,X_2,...,X_k$和对应的词性序列$Y=Y_1,Y_2,...,Y_k$，通过$X,Y$的联合概率进行建模:
<center>
  $$P(X,Y)=P(Y)P(X|Y),formula\ (1)$$
</center>
其中，
<center>
  $$P(Y)=P(Y_1)*P(Y_2|Y_1)*P(Y_3|Y_2Y_1)...*P(Y_K|Y_{K-1}...Y_1))=P(Y_1)\prod_{i=2}^K{P(Y_i|Y_{i-1}...Y{1})}$$
</center>

这里涉及到多种概率，假设X=我想学习机器学习，那么需要考虑到P(我)、P(想\|我)、P(学\|我想)、...、P(习\|我想学习机器学)等一系列的概率。这样会导致概率的稀疏性，模型复杂度也比较高。那么我们需要引入一些假设，通过这些假设来简化模型（简单的模型不一定比复杂的模型效果差，越简单的模型，一般泛化能力更好，在简单场景和小数据上表现也不差）。


这里引入HMM的第一个假设，$P(Y_i|Y_{i-1}...Y_1)=P(Y_i|Y_{i-1})$，即词性序列$Y_i$之间的一阶依赖关系，叫做一阶马尔科夫性质。更具体的说，就是当前词的词性只依赖于上一个词的词性，和其他词的词性无关。这样：
<center>
  $$P(Y)=P(Y_1)\prod_{i=2}^K{P(Y_i|Y_{i-1})}$$
</center>

对于$P(Y|X)$，引入HMM的第二个假设：$X_i$
