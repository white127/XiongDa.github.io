---
layout: post
title: "序列标志之HMM（隐马尔科夫模型）"
subtitle: "Sequence Labeling - HMM"
author: "J."
header-img: "img/post-bg-infinity.jpg"
header-mask: 0.3
mathjax: true
tags:
  - 知乎
  - Machine Learning
  - 机器学习理论
  - 序列标注
  - HMM
  - 隐马尔科夫模型
---

# 前言

序列标注（Sequence Labeling）是机器学习的一大分支，特别在NLP领域用途更广，因为人类语言是由一系列的顺序符号组成，本身就是一个文字序列。序列标注可以应用
在分词、词性标志、实体识别等领域。

序列标注有多种算法，常见的HMM（隐马尔科夫）、CRF（条件随机场）。这两个模型（包括MEMM）有非常多的相似之处和细微的差别，网上的大量文章对此的描述多是通过
大量的文字和一些简单的公式或图片来说明，细节的部分表述不够清楚。要精确的理解他们的差异，需要从算法的数学理论出发。

下面详细介绍HMM模型。

# 符号定义

为了便于理解，下面将结合一个具体的序列标注应用场景--词性标注，来进行说明。
1. $X$：变量，表示一个句子（一个字序列）
2. $X^n$：表示集合中的第n个句子，由K个字组成，$X_1^n,X_2^n,...,X_k^n$
3. $X_k^n$：表示第n个句子的第k个字，字构成句子
4. $Y$：变量，表示一个词性序列
5. $Y^n$：表示集合中的第n个句子对应的词性序列，由K个词性组成，$Y_1^n,Y_2^n,...,Y_k^n$
6. $Y_k^n$：表示第n个句子的第k个字对应的词性，所有词性构成一个词性序列
6. $D$：表示整个数据集，包括所有句子和词性序列，${\{X^n,Y^n\}}_{n=1}^N$

# 建模
对序列标注（sequence labeling）模型进行讲解，通常会使用隐藏序列和观察序列，理解起来有点抽象，为了更好的结合实际案例来理解，本文采用词性标注的应用案例。那么，对序列标注模型的基本建模思路就是：  
定义一个次序列（也就是一个句子）$X=X_1,X_2,...,X_k$和对应的词性序列$Y=Y_1,Y_2,...,Y_k$，通过$X,Y$的联合概率进行建模:
<center>
  $$P(X,Y)=P(Y)P(X|Y),formula\ (1)$$
</center>
其中，
<center>
  $$P(Y)=P(Y_1)*P(Y_2|Y_1)*P(Y_3|Y_2Y_1)...*P(Y_K|Y_{K-1}...Y_1))=P(Y_1)\prod_{i=2}^K{P(Y_i|Y_{i-1}...Y{1})}$$
</center>

这里涉及到多种概率，假设X=我想学习机器学习，那么需要考虑到P(我)、P(想\|我)、P(学\|我想)、...、P(习\|我想学习机器学)等一系列的概率。这样会导致概率的稀疏性，模型复杂度也比较高。那么我们需要引入一些假设，通过这些假设来简化模型（简单的模型不一定比复杂的模型效果差，越简单的模型，一般泛化能力更好，在简单场景和小数据上表现也不差）。


这里引入HMM的第一个假设，$P(Y_i|Y_{i-1}...Y_1)=P(Y_i\|Y_{i-1})$，即词性序列$Y_i$之间的一阶依赖关系，叫做一阶马尔科夫性质。更具体的说，就是当前词的词性只依赖于上一个词的词性，和其他词的词性无关。这样：
<center>
  $$P(Y)=P(Y_1)\prod_{i=2}^K{P(Y_i|Y_{i-1})}$$
</center>

为了让上面的公式看起来更加简洁，引入$Y_0$，满足$P(Y_1\|Y_0)=P(Y_1)$，那么上面的公式写为：
<center>
  $$P(Y)=\prod_{i=1}^K{P(Y_i|Y_{i-1})}$$
</center>

对于$P(Y\|X)$，引入HMM的第二个假设：$X_i$只和$Y_i$相关（这里隐含的意思是，除了$Y_i$之外，$X_i$和其他特征或状态都是无关的，比如，$X_i$和$X_{i+1}$是无关的。在后面的MEMM和CRF模型中，能看到他们和HMM在独立性假设上的差异），那么：
<center>
  $$P(X|Y)=\prod_{i=1}^K{P(X_i|Y_kY_{k-1}...Y_1)}$$
</center>
<center>
  $$=\prod_{i=1}^K{P(X_i|Y_i)}$$
</center>

这样，HMM在两个假设的前提下，简化为：
<center>
  $$P(X,Y)=\prod_{i=1}^K{P(Y_i|Y_{i-1})}\prod_{i=1}^K{P(X_i|Y_i)}$$
</center>

对于所有的N个样本数据，HMM的最大似然目标是需要所有样本的$P(X,Y)$的概率乘积最大。这是一种机器学习领应用非常多的最大似然参数估计方法，假设每个样本$(X,Y)$之间是相互独立的，那么N个样本的概率分布就是每个样本的概率分布的乘积，通过最大化这个概率乘积，能得到全局最优的概率分布参数：
<center>
  $$\prod_{n=1}^N{P(X^{(n)},Y^{(n)})}=\prod_{n=1}^N({\prod_{i=1}^K{P(Y_i^{(n)}|Y_{i-1}^{(n)})}\prod_{i=1}^K{P(X_i^{(n)}|Y_i^{(n)})}})$$
</center>

为了下面书写更简洁，符号做下重新定义：$t_{i,j}^{(n)}=P(Y_i^{(n)}\|Y_j^{(n)})$，$e_{i,j}^n=P(X_i^{(n)}\|Y_j^{(n)})$。

下面开始构造数学化的目标函数。上面的推导过程其实隐含了一些条件：  
1.$t_{i,j}$是一个概率分布，所以：$0<=t_{i,j}<=1$，$\sum_{i\in}{t_{i,j}=1}$
